import { getContext } from '@/utils/context'
import prismadb from '../../lib/prismadb'
import { auth, currentUser } from "@clerk/nextjs";
import { NextResponse } from 'next/server';
import { Role } from '@prisma/client';
import { checkMessageCredits, deductMessageCredit, getMessageCreditCount } from '@/lib/messageCredits';
import { streamText, StreamingTextResponse } from 'ai';
import { openai } from '@ai-sdk/openai';
import { createOpenAI } from '@ai-sdk/openai';
import OpenAI from 'openai';
import { OpenAIStream } from 'ai';

// Initialize OpenAI client
const openaiClient = new OpenAI({
  apiKey: process.env.OPENAI_API_KEY,
});

// Initialize Fireworks client as fallback
const fireworks = createOpenAI({
  apiKey: process.env.FIREWORKS_API_KEY ?? '',
  baseURL: 'https://api.fireworks.ai/inference/v1',
});

export async function POST(req: Request) {
  try {
    const {userId} = auth()

    if (!userId) {
      return new NextResponse("Unauthorized", {status: 401})
    }

    const user = await currentUser();
    if (!user || !user.id ) {
      return new NextResponse("Unauthorized", { status: 401 });
    }

    const userName = user?.firstName + " " + user?.lastName;
    const userEmail = user?.emailAddresses[0].emailAddress;

    // Fetch user settings to get the language preference
    const userSettings = await prismadb.settings.findUnique({
      where: { userId },
    });

    const outputLanguage = userSettings?.outputLanguage || "English";

    // Check if the user has credits record in the database
    const inMessageCreditsDb = await checkMessageCredits(userId);

    if (!inMessageCreditsDb) {
      return new NextResponse("Not inside credits database", {status: 401})
    } 

    const creditsLeft = await getMessageCreditCount(userId)

    if (creditsLeft ==0 || creditsLeft == false || creditsLeft < 0) {
      return new NextResponse("No more credits. Please upgrade or buy more credits.", {status: 403})
    }

    if (creditsLeft > 0) {
      console.log("Message is permitted. Deduct 1 credit after completion.")
    }

    const { messages, filter, searchResultId, countryOption } = await req.json()

    console.log("\nğŸ’¬ CHAT STATUS:");
    console.log("Number of messages in chat:", messages.length);
    console.log("Latest message from:", messages[messages.length - 1].role);

    const lastMessage = messages[messages.length - 1]

    const context = await getContext(lastMessage.content, '', filter, countryOption)

    // First LLM response (comprehensive analysis)
    const englishFirstPrompt = "I am analyzing the case with neutral citation " + filter + " in response to the following query:\n\n" +
      "USER QUERY:\n" + lastMessage.content + "\n\n" +
      "Here is the context information from this specific case:\n" +
      "START CASE CONTEXT BLOCK\n" +
      context +
      "\nEND CASE CONTEXT BLOCK\n" +
      "Please analyze this specific case for me as follows:\n\n" +
      "1. Based ONLY on the case context information provided above, summarize THIS SPECIFIC CASE concisely. Your summary must be unique to this case and based solely on the facts presented in the case context block. DO NOT reuse summaries from other cases.\n\n" +
      "2. Explain why this case is relevant to the user's query in point form. For each point:\n" +
      "   - State the reason for relevance to the user's specific query\n" +
      "   - Support your point by citing the exact relevant text from the case\n\n" +
      "3. Based on this specific case, provide a direct answer to the user's query:\n" +
      "   - What insights or guidance does this case provide for the user's situation?\n" +
      "   - What specific principles or findings from this case can be applied?\n" +
      "   - What practical advice can be drawn from this case?\n\n" +
      "Your response will be rendered using React Markdown. Use the following formatting:\n\n" +
      "- Use standard markdown for headings and lists\n" +
      "- Enclose quotes in triple backticks to render them as distinct blocks\n" +
      "- Use bold for emphasis where appropriate\n\n" +
      "Format your response like this:\n\n" +
      "## Summary\n" +
      "(Your concise summary of THIS SPECIFIC case, based only on the case context provided)\n\n" +
      "## Relevance to Your Query\n\n" +
      "### 1. First reason for relevance\n" +
      "```\n" +
      "Exact quote from the case\n" +
      "```\n" +
      "### 2. Second reason for relevance\n" +
      "```\n" +
      "Another exact quote from the case\n" +
      "```\n" +
      "(Continue with additional numbered points as needed)\n\n" +
      "## Answer to Your Query\n" +
      "(Provide a detailed answer to the user's query, using this specific case as a reference and guide. Be practical and specific in your advice.)";

    // Subsequent messages (focused response)
    const englishFollowUpPrompt = "I am continuing our discussion about the case with neutral citation " + filter + ". Here is your query:\n\n" +
      "USER QUERY:\n" + lastMessage.content + "\n\n" +
      "Here is the context information from this case:\n" +
      "START CASE CONTEXT BLOCK\n" +
      context +
      "\nEND CASE CONTEXT BLOCK\n\n" +
      "Please answer the query based on the context information provided above. Use relevant quotes from the case to support your answer.\n\n" +
      "Format your response using markdown:\n" +
      "- Use headings and lists as needed\n" +
      "- Enclose case quotes in triple backticks\n" +
      "- Use bold for emphasis where appropriate";

    // First LLM response in Chinese
    const chineseFirstPrompt = "æˆ‘æ­£åœ¨åˆ†ææ¡ˆä¾‹ç·¨è™Ÿ " + filter + "ï¼Œä»¥å›æ‡‰ä»¥ä¸‹æŸ¥è©¢ï¼š\n\n" +
      "ç”¨æˆ¶æŸ¥è©¢ï¼š\n" + lastMessage.content + "\n\n" +
      "ä»¥ä¸‹æ˜¯é€™å€‹ç‰¹å®šæ¡ˆä¾‹çš„èƒŒæ™¯è³‡è¨Šï¼ˆä»¥è‹±æ–‡æä¾›ï¼‰ï¼š\n" +
      "é–‹å§‹æ¡ˆä¾‹èƒŒæ™¯è³‡è¨Š\n" +
      context +
      "\nçµæŸæ¡ˆä¾‹èƒŒæ™¯è³‡è¨Š\n" +
      "è«‹æŒ‰ç…§ä»¥ä¸‹æ–¹å¼åˆ†æé€™å€‹ç‰¹å®šæ¡ˆä¾‹ï¼Œä¸¦ä»¥ç¹é«”ä¸­æ–‡å›ç­”ï¼š\n\n" +
      "1. åƒ…æ ¹æ“šä¸Šè¿°æ¡ˆä¾‹èƒŒæ™¯è³‡è¨Šï¼Œç°¡æ½”åœ°ç¸½çµé€™å€‹ç‰¹å®šæ¡ˆä¾‹ã€‚ä½ çš„ç¸½çµå¿…é ˆæ˜¯é‡å°é€™å€‹æ¡ˆä¾‹çš„ç¨ç‰¹å…§å®¹ï¼Œä¸¦ä¸”åªåŸºæ–¼èƒŒæ™¯è³‡è¨Šä¸­å‘ˆç¾çš„äº‹å¯¦ã€‚ä¸è¦é‡è¤‡ä½¿ç”¨å…¶ä»–æ¡ˆä¾‹çš„ç¸½çµã€‚\n\n" +
      "2. ä»¥è¦é»å½¢å¼è§£é‡‹ç‚ºä»€éº¼é€™å€‹æ¡ˆä¾‹èˆ‡ç”¨æˆ¶çš„æŸ¥è©¢ç›¸é—œã€‚å°æ–¼æ¯å€‹è¦é»ï¼š\n" +
      "   - èªªæ˜èˆ‡ç”¨æˆ¶å…·é«”æŸ¥è©¢çš„ç›¸é—œæ€§åŸå› \n" +
      "   - å¼•ç”¨æ¡ˆä¾‹ä¸­çš„ç¢ºåˆ‡ç›¸é—œæ–‡å­—ï¼ˆä¿æŒè‹±æ–‡åŸæ–‡ï¼‰ä¾†æ”¯æŒä½ çš„è§€é»\n\n" +
      "3. æ ¹æ“šé€™å€‹ç‰¹å®šæ¡ˆä¾‹ï¼Œç›´æ¥å›ç­”ç”¨æˆ¶çš„æŸ¥è©¢ï¼š\n" +
      "   - é€™å€‹æ¡ˆä¾‹å°ç”¨æˆ¶çš„æƒ…æ³æä¾›äº†ä»€éº¼è¦‹è§£æˆ–æŒ‡å°ï¼Ÿ\n" +
      "   - é€™å€‹æ¡ˆä¾‹ä¸­æœ‰ä»€éº¼å…·é«”åŸå‰‡æˆ–çµæœå¯ä»¥æ‡‰ç”¨ï¼Ÿ\n" +
      "   - å¾é€™å€‹æ¡ˆä¾‹å¯ä»¥å¾—å‡ºä»€éº¼å¯¦ç”¨å»ºè­°ï¼Ÿ\n\n" +
      "ä½ çš„å›ç­”å°‡ä½¿ç”¨ React Markdown å‘ˆç¾ã€‚è«‹ä½¿ç”¨ä»¥ä¸‹æ ¼å¼ï¼š\n\n" +
      "- ä½¿ç”¨æ¨™æº– markdown èªæ³•ä¾†è¡¨ç¤ºæ¨™é¡Œå’Œåˆ—è¡¨\n" +
      "- ä½¿ç”¨ä¸‰å€‹åå¼•è™Ÿå°‡å¼•ç”¨åŒ…åœï¼Œä»¥ä¾¿å°‡å…¶å‘ˆç¾ç‚ºç¨ç‰¹çš„å€å¡Š\n" +
      "- é©ç•¶ä½¿ç”¨ç²—é«”ä¾†å¼·èª¿\n\n" +
      "è«‹æŒ‰ç…§ä»¥ä¸‹æ ¼å¼å›ç­”ï¼ˆé™¤äº†å¼•ç”¨å¤–ï¼Œå…¶é¤˜éƒ¨åˆ†è«‹ä½¿ç”¨ç¹é«”ä¸­æ–‡ï¼‰ï¼š\n\n" +
      "## æ‘˜è¦\n" +
      "ï¼ˆä½ å°é€™å€‹ç‰¹å®šæ¡ˆä¾‹çš„ç°¡æ½”ç¸½çµï¼Œåƒ…åŸºæ–¼æä¾›çš„æ¡ˆä¾‹èƒŒæ™¯è³‡è¨Šï¼‰\n\n" +
      "## èˆ‡ä½ æŸ¥è©¢çš„ç›¸é—œæ€§\n\n" +
      "### 1. ç¬¬ä¸€å€‹ç›¸é—œæ€§åŸå› \n" +
      "```\n" +
      "æ¡ˆä¾‹ä¸­çš„ç¢ºåˆ‡å¼•ç”¨ï¼ˆä¿æŒè‹±æ–‡åŸæ–‡ï¼‰\n" +
      "```\n" +
      "### 2. ç¬¬äºŒå€‹ç›¸é—œæ€§åŸå› \n" +
      "```\n" +
      "æ¡ˆä¾‹ä¸­çš„å¦ä¸€å€‹ç¢ºåˆ‡å¼•ç”¨ï¼ˆä¿æŒè‹±æ–‡åŸæ–‡ï¼‰\n" +
      "```\n" +
      "ï¼ˆæ ¹æ“šéœ€è¦ç¹¼çºŒæ·»åŠ ç·¨è™Ÿçš„è¦é»ï¼‰\n\n" +
      "## å›æ‡‰ä½ çš„æŸ¥è©¢\n" +
      "ï¼ˆæ ¹æ“šé€™å€‹ç‰¹å®šæ¡ˆä¾‹ç‚ºåƒè€ƒå’ŒæŒ‡å°ï¼Œè©³ç´°å›ç­”ç”¨æˆ¶çš„æŸ¥è©¢ã€‚æä¾›å¯¦ç”¨å’Œå…·é«”çš„å»ºè­°ã€‚ï¼‰";

    // Subsequent messages in Chinese
    const chineseFollowUpPrompt = "æˆ‘å€‘æ­£åœ¨è¨è«–æ¡ˆä¾‹ç·¨è™Ÿ " + filter + "ã€‚ä»¥ä¸‹æ˜¯ä½ çš„æŸ¥è©¢ï¼š\n\n" +
      "ç”¨æˆ¶æŸ¥è©¢ï¼š\n" + lastMessage.content + "\n\n" +
      "ä»¥ä¸‹æ˜¯é€™å€‹æ¡ˆä¾‹çš„èƒŒæ™¯è³‡è¨Šï¼ˆä»¥è‹±æ–‡æä¾›ï¼‰ï¼š\n" +
      "é–‹å§‹æ¡ˆä¾‹èƒŒæ™¯è³‡è¨Š\n" +
      context +
      "\nçµæŸæ¡ˆä¾‹èƒŒæ™¯è³‡è¨Š\n\n" +
      "è«‹æ ¹æ“šä¸Šè¿°èƒŒæ™¯è³‡è¨Šå›ç­”æŸ¥è©¢ã€‚å¼•ç”¨æ¡ˆä¾‹ä¸­çš„ç›¸é—œå…§å®¹ï¼ˆä¿æŒè‹±æ–‡åŸæ–‡ï¼‰ä¾†æ”¯æŒä½ çš„ç­”æ¡ˆã€‚\n\n" +
      "è«‹ä½¿ç”¨ä»¥ä¸‹æ ¼å¼ï¼ˆé™¤äº†å¼•ç”¨å¤–ï¼Œå…¶é¤˜éƒ¨åˆ†è«‹ä½¿ç”¨ç¹é«”ä¸­æ–‡ï¼‰ï¼š\n" +
      "- ä½¿ç”¨æ¨™é¡Œå’Œåˆ—è¡¨\n" +
      "- ä½¿ç”¨ä¸‰å€‹åå¼•è™ŸåŒ…åœæ¡ˆä¾‹å¼•ç”¨\n" +
      "- é©ç•¶ä½¿ç”¨ç²—é«”ä¾†å¼·èª¿";

    // Select appropriate prompt based on message count and language
    const selectedPrompt = messages.length === 1 
      ? (outputLanguage === "English" ? englishFirstPrompt : chineseFirstPrompt)
      : (outputLanguage === "English" ? englishFollowUpPrompt : chineseFollowUpPrompt);

    console.log("\nğŸ¤– PROMPT SENT TO LLM:");
    console.log("=".repeat(80));
    console.log(selectedPrompt);
    console.log("=".repeat(80));

    let initialPrompt = [
      {
        role: 'user',
        content: selectedPrompt
      }
    ];

    try {
      // Try OpenAI GPT-4o-mini first
      const response = await openaiClient.chat.completions.create({
        model: 'gpt-4o-mini',
        messages: [
          {
            role: 'system',
            content: "You are a legal assistant analyzing case law. Provide detailed analysis and practical insights."
          },
          {
            role: 'user',
            content: initialPrompt[0].content
          },
          ...messages
        ],
        stream: true,
      });

      const stream = OpenAIStream(response, {
        async onCompletion(completion) {
          console.log("\nğŸ¤– RAW LLM RESPONSE:");
          console.log("=".repeat(80));
          console.log(completion);
          console.log("=".repeat(80));

          await prismadb.message.create({
            data:{
              role: Role.assistant,
              content: completion,
              userId: user.id,
              searchResultId: searchResultId,
              userName: userName,
              userEmail: userEmail
            }
          });
          await deductMessageCredit(userId)
        }
      });

      return new StreamingTextResponse(stream);

    } catch (error) {
      console.log("OpenAI error, falling back to Fireworks:", error);
      
      // Fallback to Fireworks
      const model = fireworks('accounts/fireworks/models/llama-v3-70b-instruct');

      const result = await streamText({
        model: model,
        system: "You are a legal assistant analyzing case law. Provide detailed analysis and practical insights.",
        messages: messages,
        maxTokens: 1000
      });

      const stream = result.toAIStream({
        async onStart()  {
          await prismadb.message.create({
            data:{
              role: Role.user,
              content: lastMessage.content,
              userId: user.id,
              searchResultId: searchResultId,
              userName: userName,
              userEmail: userEmail
            }
          });
        },
        async onCompletion(completion: string) {
          console.log("\nğŸ¤– RAW LLM RESPONSE (Fireworks fallback):");
          console.log("=".repeat(80));
          console.log(completion);
          console.log("=".repeat(80));

          await prismadb.message.create({
            data:{
              role: Role.assistant,
              content: completion,
              userId: user.id,
              searchResultId: searchResultId,
              userName: userName,
              userEmail: userEmail
            }
          });
          await deductMessageCredit(userId)
        }
      });

      return new StreamingTextResponse(stream);
    }

  } catch (e) {
    throw (e)
  }
}